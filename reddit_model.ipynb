{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o24.addFile.\n: java.io.FileNotFoundException: File file:/C:/Users/Nabeel/Documents/Python%20Scripts/Project2/cleantext.py does not exist\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)\r\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1528)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5e7475800945>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msc\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msqlContext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddPyFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cleantext.py\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m#main(sqlContext)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36maddPyFile\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mHTTP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHTTPS\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mFTP\u001b[0m \u001b[0mURI\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \"\"\"\n\u001b[1;32m--> 859\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# dirname may be directory or HDFS/S3 prefix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPACKAGE_EXTENSIONS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36maddFile\u001b[1;34m(self, path, recursive)\u001b[0m\n\u001b[0;32m    848\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m400\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m         \"\"\"\n\u001b[1;32m--> 850\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    851\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    852\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0maddPyFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1160\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1162\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    319\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    321\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o24.addFile.\n: java.io.FileNotFoundException: File file:/C:/Users/Nabeel/Documents/Python%20Scripts/Project2/cleantext.py does not exist\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)\r\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1528)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setAppName(\"CS143 Project 2B\")\n",
    "conf = conf.setMaster(\"local[*]\")\n",
    "sc   = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "sc.addPyFile(\"cleantext.py\")\n",
    "#main(sqlContext)\n",
    "\n",
    "comments = sqlContext.read.json(\"comments-minimal.json.bz2\")\n",
    "#comments.write.parquet(\"comments-minimal.json.bz2\")\n",
    "#submissions = sqlContext.read.json(\"submissions.json.bz2\")\n",
    "labels = sqlContext.read.csv(\"labeled_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "1. The functional dependencies on labeled_data.csv is input_id -> (labeldem, labelgop, labeldjt). The input_id is a superkey hence this is in BCNF. \n",
    "\n",
    "2. Comments has the schema (Author(char), Body(text), Can_Gild(boolean), Collapsed(Boolean), Contraversality(int), created_utc(int?), edited(boolean), gilded(int), id(char), is_submitter(boolean), link_id(char), parent_id(char), retrieved_on(int?), score(int), stickied(boolean), subreddit(text). We see there is some redundancy. For example, author determines Can_Gild. However, there don't seem to be many redundancies overall.\n",
    "\n",
    "\n",
    "Write SQL query: SELECT comment, input_id, labeldel, labelgop, labeldjt \n",
    "                 FROM comment C INNER JOIN labeled_data L ON C.id = L.input_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.printSchema()\n",
    "comments.printSchema()\n",
    "#labels.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.createOrReplaceTempView(\"Comments\")\n",
    "labels.createOrReplaceTempView(\"Labels\")\n",
    "\n",
    "X_y = sqlContext.sql(\"SELECT C.body, L._c0, L._c1, L._c2, L._c3 FROM Comments C INNER JOIN Labels L ON C.id = L._c0\")\n",
    "\n",
    "#X_y.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleantext import sanitize\n",
    "\n",
    "X_y.createOrReplaceTempView(\"Train\")\n",
    "just_comm = sqlContext.sql(\"SELECT body FROM Train\")\n",
    "#just_comm = just_comm.astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType\n",
    "from pyspark.sql.types import StringType\n",
    "#add the n_grams\n",
    "from pyspark.sql.functions import udf\n",
    "sanitize_udf = udf(sanitize)\n",
    "sanitized_df = X_y.select(\"body\", \"_c1\", \"_c2\", \"_c3\", sanitize_udf(\"body\").alias(\"n_grams\"))\n",
    "\n",
    "concat_string_arrays_udf = udf(lambda out: (' '.join(out)).split(), ArrayType(StringType(), True))\n",
    "concat_string_arrays_df = sanitized_df.select(\"body\", \"_c1\", \"_c2\", \"_c3\", concat_string_arrays_udf(\"n_grams\").alias(\"n_grams_formatted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_string_arrays_df.limit(2).show(2, False)\n",
    "#sanitized_df.show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count vectorizer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"n_grams_formatted\", outputCol=\"features\", minDF = 5, binary=True)\n",
    "\n",
    "model = cv.fit(concat_string_arrays_df)\n",
    "\n",
    "#result = model.transform(comments_df)\n",
    "#result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.transform(concat_string_arrays_df)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "pos = result.select(\"body\", \"features\", F.when(X_y._c1 == 1, 1).otherwise(0).alias(\"dem\"), F.when(X_y._c2 == 1, 1).otherwise(0).alias(\"gop\"), F.when(X_y._c3 == 1, 1).otherwise(0).alias(\"label\"))\n",
    "pos.show(20)\n",
    "neg = result.select(\"body\", \"features\", F.when(X_y._c1 == -1, 1).otherwise(0).alias(\"dem\"), F.when(X_y._c2 == -1, 1).otherwise(0).alias(\"gop\"), F.when(X_y._c3 == -1, 1).otherwise(0).alias(\"label\"))\n",
    "neg.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bunch of imports (may need more)\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "# Initialize two logistic regression models.\n",
    "# Replace labelCol with the column containing the label, and featuresCol with the column containing the features.\n",
    "poslr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10).setThreshold(.2)\n",
    "neglr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10).setThreshold(.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a binary classifier so we need an evaluator that knows how to deal with binary classifiers.\n",
    "posEvaluator = BinaryClassificationEvaluator()\n",
    "negEvaluator = BinaryClassificationEvaluator()\n",
    "# There are a few parameters associated with logistic regression. We do not know what they are a priori.\n",
    "# We do a grid search to find the best parameters. We can replace [1.0] with a list of values to try.\n",
    "# We will assume the parameter is 1.0. Grid search takes forever.\n",
    "posParamGrid = ParamGridBuilder().addGrid(poslr.regParam, [1.0]).build()\n",
    "negParamGrid = ParamGridBuilder().addGrid(neglr.regParam, [1.0]).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We initialize a 5 fold cross-validation pipeline.\n",
    "posCrossval = CrossValidator(\n",
    "    estimator=poslr,\n",
    "    evaluator=posEvaluator,\n",
    "    estimatorParamMaps=posParamGrid,\n",
    "    numFolds=5)\n",
    "negCrossval = CrossValidator(\n",
    "    estimator=neglr,\n",
    "    evaluator=negEvaluator,\n",
    "    estimatorParamMaps=negParamGrid,\n",
    "    numFolds=5)\n",
    "# Although crossvalidation creates its own train/test sets for\n",
    "# tuning, we still need a labeled test set, because it is not\n",
    "# accessible from the crossvalidator (argh!)\n",
    "# Split the data 50/50\n",
    "posTrain, posTest = pos.randomSplit([0.5, 0.5])\n",
    "negTrain, negTest = neg.randomSplit([0.5, 0.5])\n",
    "# Train the models\n",
    "print(\"Training positive classifier...\")\n",
    "posModel = posCrossval.fit(posTrain)\n",
    "print(\"Training negative classifier...\")\n",
    "negModel = negCrossval.fit(negTrain)\n",
    "\n",
    "# Once we train the models, we don't want to do it again. We can save the models and load them again later.\n",
    "# posModel.save(\"www/pos.model\")\n",
    "# negModel.save(\"www/neg.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtains data for testing\n",
    "comments = sqlContext.read.json(\"comments-minimal.json.bz2\")\n",
    "comments.createOrReplaceTempView(\"Comments\")\n",
    "#comments.printSchema()\n",
    "submissions = sqlContext.read.json(\"submissions.json.bz2\")\n",
    "submissions.createOrReplaceTempView(\"Submissions\")\n",
    "#submissions.printSchema()\n",
    "#submissions.limit(1).show()\n",
    "\n",
    "eval_data = sqlContext.sql(\"SELECT Comments.body as body, Comments.id as id FROM Comments JOIN Submissions ON Comments.link_id = Concat('t3_', Submissions.id)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Task 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepares data for testing\n",
    "\n",
    "eval_data.createOrReplaceTempView(\"Eval_data\")\n",
    "#eval_data = sqlContext.sql(\"SELECT * FROM Eval_data\")\n",
    "eval_data = sqlContext.sql(\"SELECT body, id FROM Eval_data WHERE Eval_data.body NOT LIKE '/s' AND Eval_data.body NOT LIKE '&gt'\")\n",
    "\n",
    "#eval_data.where(not(\"/s\" in eval_data.body) and not(\"&gt\" in eval_data.body))\n",
    "\n",
    "sanitize_udf = udf(sanitize)\n",
    "#eval_data.show()\n",
    "sanitized_df = eval_data.select(\"body\", \"id\", sanitize_udf(\"body\").alias(\"n_grams\"))\n",
    "\n",
    "concat_string_arrays_udf = udf(lambda out: (' '.join(out)).split(), ArrayType(StringType(), True))\n",
    "concat_string_arrays_df = sanitized_df.select(\"body\", \"id\", concat_string_arrays_udf(\"n_grams\").alias(\"n_grams_formatted\"))\n",
    "\n",
    "#posTest = concat_string_arrays_df.select(\"body\", \"state\", \"created_utc\", \"score\", \"controversiality\", \"distinguished\", \"retrieved_on\", \"n_grams_formatted\", F.when(concat_string_arrays_df._c3 == 1, 1).otherwise(0).alias(\"label\"))\n",
    "#negTest =  concat_string_arrays_df.select(\"body\", \"state\", \"created_utc\", \"score\", \"controversiality\", \"distinguished\", \"retrieved_on\", \"n_grams_formatted\", F.when(concat_string_arrays_df._c3 == -1, 1).otherwise(0).alias(\"label\"))\n",
    "\n",
    "\n",
    "#run the model on the data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "posTestCounts = model.transform(concat_string_arrays_df)\n",
    "posResult = posModel.transform(posTestCounts)\n",
    "negTestCounts = model.transform(concat_string_arrays_df)\n",
    "negResult = negModel.transform(negTestCounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "posResult.show()\n",
    "#posResult.write.format(\"posResult.csv\")\n",
    "negResult.show()\n",
    "#negResult.write.format(\"negResult.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posResult.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posResult.createOrReplaceTempView(\"PosResult\")\n",
    "negResult.createOrReplaceTempView(\"NegResult\")\n",
    "resultCount = sqlContext.sql(\"SELECT COUNT(*) FROM PosResult\")\n",
    "resultCount.show()\n",
    "posResultCount = sqlContext.sql(\"SELECT COUNT(*) FROM PosResult WHERE prediction == 1\")\n",
    "posResultCount.show()\n",
    "negResultCount = sqlContext.sql(\"SELECT COUNT(*) FROM NegResult WHERE prediction == 1\")\n",
    "negResultCount.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.createOrReplaceTempView(\"Comments\")\n",
    "posResultWComment = sqlContext.sql(\"SELECT Comments.body as body, Comments.id as id, Comments.author_flair_text as state, Comments.created_utc as created_utc, Comments.score as score, Comments.controversiality as controversiality, Comments.distinguished as distinguished, PosResult.prediction as prediction FROM Comments JOIN PosResult ON Comments.id == PosResult.id\")\n",
    "posResultWComment.createOrReplaceTempView(\"PosResultWComment\")\n",
    "negResultWComment = sqlContext.sql(\"SELECT Comments.body as body, Comments.id as id, Comments.author_flair_text as state, Comments.created_utc as created_utc, Comments.score as score, Comments.controversiality as controversiality, Comments.distinguished as distinguished, NegResult.prediction as prediction FROM Comments JOIN NegResult ON Comments.id == PosResult.id\")\n",
    "negResultWComment.createOrReplaceTempView(\"NegResultWComment\")\n",
    "#percent by day\n",
    "posDays = sqlContext.sql(\"SELECT created_utc, SUM(prediction) / COUNT(prediction) as pct FROM PosResultWComment GROUP BY created_utc\")\n",
    "posDays.show()\n",
    "negDays = sqlContext.sql(\"SELECT created_utc, SUM(prediction) / COUNT(prediction) as pct FROM NegResultWComment GROUP BY created_utc\")\n",
    "negDays.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posState = sqlContext.sql(\"SELECT state, SUM(prediction) / COUNT(prediction) as pct FROM PosResultWComment GROUP BY state\")\n",
    "posState.show()\n",
    "negState = sqlContext.sql(\"SELECT state, SUM(prediction) / COUNT(prediction) as pct FROM NegResultWComment GROUP BY state\")\n",
    "negState.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posScores = sqlContext.sql(\"SELECT id, SUM(prediction) / COUNT(prediction) as pct FROM PosResultWComment GROUP BY id\")\n",
    "posScores.show()\n",
    "negScores = sqlContext.sql(\"SELECT id, SUM(prediction) / COUNT(prediction) as pct FROM NegResultWComment GROUP BY id\")\n",
    "negScores.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posScores = sqlContext.sql(\"SELECT score, SUM(prediction) / COUNT(prediction) as pct FROM PosResultWComment GROUP BY score\")\n",
    "posScores.show()\n",
    "negScores = sqlContext.sql(\"SELECT score, SUM(prediction) / COUNT(prediction) as pct FROM NegResultWComment GROUP BY score\")\n",
    "negScores.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posControversiality = sqlContext.sql(\"SELECT controversiality, SUM(prediction) / COUNT(prediction) as pct FROM PosResultWComment GROUP BY controversiality\")\n",
    "posControversiality.show()\n",
    "negControversiality = sqlContext.sql(\"SELECT controversiality, SUM(prediction) / COUNT(prediction) as pct FROM NegResultWComment GROUP BY controversiality\")\n",
    "negControversiality.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posDistinguished = sqlContext.sql(\"SELECT distinguished, SUM(prediction) / COUNT(prediction) as pct FROM PosResultWComment GROUP BY distinguished\")\n",
    "posDistinguished.show()\n",
    "negDistinguished = sqlContext.sql(\"SELECT distinguished, SUM(prediction) / COUNT(prediction) as pct FROM NegResultWComment GROUP BY distinguished\")\n",
    "negDistinguished.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
